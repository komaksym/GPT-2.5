{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e52b5cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "<|endoftext|>\n",
      "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
      "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
      "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\n",
      "\"Tom, can I have some blocks too?\" Lily asked. She wanted to make a bridge for her cars.\n",
      "\"No, these are mine. Go find your own,\" Tom said. He did not want to share with his sister. He pulled the blocks closer to him.\n",
      "Lily felt sad and angry. She did not think Tom was being nice. She looked at his tower and had an idea. She decided to pull one of the blocks at the bottom of the tower.\n",
      "Suddenly, the tower fell down with a loud crash. All the blocks and cars scattered on the floor. Tom and Lily were shocked. They felt the floor shake and heard a rumble. It was an earthquake!\n",
      "\"Mommy! Daddy!\" they cried. They were scared and ran to their parents, who were in the kitchen.\n",
      "\"Are you okay, kids?\" Mommy asked. She hugged them and checked if they were hurt.\n",
      "\"We're okay, Mommy. But our toys are broken,\" Lily said.\n",
      "\"I'm sorry, Lily. But toys are not important. You are important. We are safe and together. That's what matters,\" Mommy said.\n",
      "Tom felt sorry for what he did. He realized he was selfish and mean to his sister. He saw how scared she was during the earthquake. He wanted to make her happy.\n",
      "\"Lily, I'm sorry I did not share with you. You can have all the blocks you want. I love you, sister,\" Tom said.\n",
      "Lily smiled and hugged him. She forgave him and thanked him. She loved him too.\n",
      "They went back to the living room and cleaned up their toys. They decided to build something together. They made a big house with a garden and a fence. They put their cars and dolls inside. They were happy and proud of their work.\n",
      "Mommy and Daddy came to see their house. They praised them and gave them a treat. It was a lemon cake. It was sour, but they liked it. They learned that sharing is\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/TinyStoriesV2-GPT4-valid.txt\", 'r') as f:\n",
    "    corpus = f.read()[:3000]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4157736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dfd8d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def strip_of_special_tokens(corpus, special_tokens):\n",
    "    \"\"\"Strips of special tokens to avoid counting them as bytes\"\"\"\n",
    "\n",
    "    # Escape | delimiter in special tokens\n",
    "    for i in range(len(special_tokens)):\n",
    "        if \"|\" in special_tokens[i]:\n",
    "            special_tokens[i] = re.escape(special_tokens[i])\n",
    "\n",
    "    # Join special tokens into a delim for a splitting pattern\n",
    "    delim = \"|\".join(special_tokens)\n",
    "    chunks = re.split(delim, corpus)\n",
    "    # Remove empty chunks\n",
    "    chunks = [ch for ch in chunks if ch.strip()]\n",
    "    return chunks\n",
    "\n",
    "special_tokens = [\"<|endoftext|>\", \"<start>\", \"<end>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dfb24bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 262\n",
    "num_of_merges = vocab_size - 256\n",
    "vocab = {}\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# Pre-tokenization\n",
    "def pretokenize(corpus, ptrn):\n",
    "    \"\"\"Pre-tokenizes on regex pattern\"\"\"\n",
    "\n",
    "    counts = {}\n",
    "    for t in corpus:\n",
    "        for word in re.findall(ptrn, t):\n",
    "            counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "42048bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_bytes(corpus):\n",
    "    \"\"\"Splits words by characters and counts frequency\"\"\"\n",
    "\n",
    "    counts = {}\n",
    "    # Count byte pairs\n",
    "    for k, v in corpus.items():\n",
    "        new_key = tuple([c for c in k])\n",
    "        counts[new_key] = v\n",
    "\n",
    "    # Sort by the highest frequency\n",
    "    counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "195ae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_bytepairs(corpus, bp_to_counts=None, bp_to_words=None, mf_pair=None, merged_words=None):\n",
    "    \"\"\"Counts bytepair frequencies in the corpus\n",
    "    If ran the first time (no counts provided) then count all byte pairs in the whole corpus\n",
    "    If ran consecutively, remove the most frequent pair count as it's merged now\n",
    "    and count byte pairs only on the merged words now\"\"\"\n",
    "\n",
    "    # If counts are provided, remove the most frequent byte pair from the counts and then re-count only the merged words\n",
    "    if bp_to_counts and bp_to_words:\n",
    "        # Collect byte pairs to remove which overlap with pre-merge bytes\n",
    "        pairs_to_remove = [mf_pair]\n",
    "        pairs_to_remove.extend([p for p in bp_to_counts.keys() if p[1] == mf_pair[0] or p[0] == mf_pair[1]])\n",
    "\n",
    "        # Remove the old byte pairs that were created on pre-merge\n",
    "        for p in pairs_to_remove:\n",
    "            bp_to_counts.pop(p)\n",
    "            bp_to_words.pop(p)\n",
    "\n",
    "        # Clean up old word references: remove words that no longer exist in corpus\n",
    "        # This is necessary because merged words change their keys, leaving stale references\n",
    "        for pair in list(bp_to_words.keys()):\n",
    "            bp_to_words[pair] = {w for w in bp_to_words[pair] if w in corpus}\n",
    "            # Remove empty sets to keep things clean\n",
    "            if not bp_to_words[pair]:\n",
    "                bp_to_words.pop(pair)\n",
    "                bp_to_counts.pop(pair, None)\n",
    "\n",
    "        # Pop the old bytepair counts before counting new ones\n",
    "        for w in merged_words:\n",
    "           for c1, c2 in zip(w, w[1:]):\n",
    "                bp_to_counts[(c1, c2)] = 0\n",
    "        \n",
    "        # Count only merged words as only these are updated\n",
    "        for w in merged_words:\n",
    "           for c1, c2 in zip(w, w[1:]):\n",
    "                bp_to_counts[(c1, c2)] = bp_to_counts.get((c1, c2), 0) + corpus[w]\n",
    "                bp_to_words[(c1, c2)].add(w) \n",
    "    \n",
    "    # For the first time we need to count every single pair\n",
    "    else:\n",
    "        bp_to_counts = {}\n",
    "        bp_to_words = defaultdict(set)\n",
    "\n",
    "        # Count every single pair in the corpus\n",
    "        for k, v in corpus.items():\n",
    "            for c1, c2 in zip(k, k[1:]):\n",
    "                bp_to_counts[(c1, c2)] = bp_to_counts.get((c1, c2), 0) + v\n",
    "                bp_to_words[(c1, c2)].add(k)\n",
    "\n",
    "    return bp_to_counts, bp_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3a563cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mf_pair(counts, counts_to_words):\n",
    "    \"\"\"Takes the most frequent byte pair along with words with that pair and returns them\"\"\"\n",
    "\n",
    "    # Get the max frequency\n",
    "    maxf = counts[max(counts, key=counts.get)]\n",
    "\n",
    "    # Get the candidates with the max frequency\n",
    "    candidates = [k for k, v in counts.items() if v == maxf]\n",
    "    # Pick the lexicographically greater pair\n",
    "    pair = max(candidates)\n",
    "    return pair, counts_to_words[pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "605df658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(corpus, merge_pair, merge_pair_words):\n",
    "    \"\"\"Merges the word in the corpus \n",
    "    by joining two bytes into one and\n",
    "    re-assigning the key in the dictionary\"\"\"\n",
    "\n",
    "    # Keeps track of merged words to update byte counting only on these as these only change\n",
    "    merged_words = set()\n",
    "    \n",
    "    # Merging the keys \n",
    "    for w in merge_pair_words:\n",
    "        new_k = []\n",
    "        b = 0\n",
    "        while b < len(w):\n",
    "            if b + 1 < len(w):\n",
    "                c1, c2 = w[b], w[b+1]\n",
    "                if c1 + c2 == merge_pair:\n",
    "                    new_k.append(merge_pair)\n",
    "                    b += 2\n",
    "                else:\n",
    "                    new_k.append(c1)\n",
    "                    b += 1\n",
    "            else:\n",
    "                new_k.append(w[b])\n",
    "                b += 1\n",
    "        # Add new merged word\n",
    "        corpus[tuple(new_k)] = corpus[w]\n",
    "\n",
    "        # Add to merged words to optimize byte pair counts as this only changes and the rest is still the same\n",
    "        merged_words.add(tuple(new_k))\n",
    "    \n",
    "    # Pop the unmerged words\n",
    "    for w in merge_pair_words:\n",
    "        corpus.pop(w)\n",
    "    \n",
    "    return corpus, merged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8decf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_to_bytes(pair):\n",
    "    return tuple(b.encode(\"utf-8\") for b in pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c102f",
   "metadata": {},
   "source": [
    "## Main merging loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6378f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "mf_pair = None\n",
    "merged_words = None\n",
    "\n",
    "corpus = strip_of_special_tokens(corpus, special_tokens)\n",
    "corpus = pretokenize(corpus, PAT)\n",
    "corpus = split_to_bytes(corpus)\n",
    "\n",
    "# Start merges\n",
    "for i in range(num_of_merges):\n",
    "    if i == 0:\n",
    "        # Count bytepairs\n",
    "        counts, counts_to_words = count_bytepairs(corpus)\n",
    "    else:\n",
    "       counts, counts_to_words = count_bytepairs(corpus, counts, counts_to_words, mf_pair, merged_words) \n",
    "    # Get the most frequent pair\n",
    "    mf_pair, mf_pair_words = get_mf_pair(counts, counts_to_words)\n",
    "    # Add merge to merges\n",
    "    pair_b = pair_to_bytes(mf_pair)\n",
    "    merges.append(pair_b)\n",
    "    # Add the merge to the vocab\n",
    "    merge_b = \"\".join(mf_pair).encode(\"utf-8\")\n",
    "    vocab[256 + i] = merge_b\n",
    "    # Apply the merge to the corpus\n",
    "    corpus, merged_words = merge(corpus, \"\".join(mf_pair), mf_pair_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7a424f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: b'st', 257: b'est', 258: b'ow', 259: b'low', 260: b'ne', 261: b'west'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "127bcb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'n', b'e'),\n",
       " (b'w', b'est')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
